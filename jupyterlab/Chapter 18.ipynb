{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf80aa82-ae20-4ec4-9402-68d9ebefec1f",
   "metadata": {},
   "source": [
    "# Chapter 18\n",
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b12f0b-5244-40ae-8774-269f471720b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93320f2-9514-46fd-a48a-24f66bf96edd",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304b8ff7-d7ab-4ca6-beb7-048fc8329ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "text_2 = \"My dog is quick and can jump over fences\"\n",
    "text_3 = \"Your dog is so lazy that it sleeps all day\"\n",
    "text_4 = \"A black dog just passed by but my dog is brown\"\n",
    "corpus = [text_1, text_2, text_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4128c6b9-8059-4864-868d-956c31744efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1]]\n",
      "\n",
      "{'the': 19, 'quick': 15, 'brown': 2, 'fox': 7, 'jumps': 11, 'over': 14, 'lazy': 12, 'dog': 5, 'my': 13, 'is': 8, 'and': 1, 'can': 3, 'jump': 10, 'fences': 6, 'your': 20, 'so': 17, 'that': 18, 'it': 9, 'sleeps': 16, 'all': 0, 'day': 4}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = text.CountVectorizer(binary=True)\n",
    "vectorized_texts = vectorizer.fit_transform(corpus)\n",
    "print(vectorized_texts.todense())\n",
    "print()\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1622fc5-22ea-4b63-a158-12cd0e81caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1]\n",
      " [0 0 1 1 1 1 0 0 2 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "corpus.append(text_4)\n",
    "vectorizer = text.CountVectorizer()\n",
    "vectorized_texts = vectorizer.fit_transform(corpus)\n",
    "print(vectorized_texts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbea5ca3-29ef-40a8-beb9-7e092045e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0\n",
      "       the: 0.261\n",
      "     quick: 0.103\n",
      "     brown: 0.103\n",
      "       fox: 0.13\n",
      "     jumps: 0.13\n",
      "      over: 0.103\n",
      "      lazy: 0.103\n",
      "       dog: 0.068\n",
      "\n",
      "Text 1\n",
      "     quick: 0.105\n",
      "      over: 0.105\n",
      "       dog: 0.0693\n",
      "        my: 0.105\n",
      "        is: 0.0848\n",
      "       and: 0.133\n",
      "       can: 0.133\n",
      "      jump: 0.133\n",
      "    fences: 0.133\n",
      "\n",
      "Text 2\n",
      "      lazy: 0.0881\n",
      "       dog: 0.0583\n",
      "        is: 0.0713\n",
      "      your: 0.112\n",
      "        so: 0.112\n",
      "      that: 0.112\n",
      "        it: 0.112\n",
      "    sleeps: 0.112\n",
      "       all: 0.112\n",
      "       day: 0.112\n",
      "\n",
      "Text 3\n",
      "     brown: 0.0955\n",
      "       dog: 0.126\n",
      "        my: 0.0955\n",
      "        is: 0.0773\n",
      "     black: 0.121\n",
      "      just: 0.121\n",
      "    passed: 0.121\n",
      "        by: 0.121\n",
      "       but: 0.121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = text.TfidfTransformer(norm='l1')\n",
    "tfidf_mtx = tfidf.fit_transform(vectorized_texts)\n",
    "\n",
    "total = 0\n",
    "\n",
    "for i, phrase in enumerate(tfidf_mtx.toarray()):\n",
    "    print(f\"Text {i}\")\n",
    "    for word in vectorizer.vocabulary_:\n",
    "        pos = vectorizer.vocabulary_[word]\n",
    "        value = phrase[pos]\n",
    "        if value != 0: print(f\"{word:>10}: {value:.3}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411d94d1-b284-4071-912c-5976ad78e12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 52, 'quick': 43, 'brown': 6, 'fox': 19, 'jumps': 29, 'over': 38, 'lazy': 33, 'dog': 15, 'the quick': 54, 'quick brown': 45, 'brown fox': 7, 'fox jumps': 20, 'jumps over': 30, 'over the': 40, 'the lazy': 53, 'lazy dog': 34, 'my': 36, 'is': 21, 'and': 2, 'can': 12, 'jump': 27, 'fences': 18, 'my dog': 37, 'dog is': 16, 'is quick': 23, 'quick and': 44, 'and can': 3, 'can jump': 13, 'jump over': 28, 'over fences': 39, 'your': 55, 'so': 48, 'that': 50, 'it': 25, 'sleeps': 46, 'all': 0, 'day': 14, 'your dog': 56, 'is so': 24, 'so lazy': 49, 'lazy that': 35, 'that it': 51, 'it sleeps': 26, 'sleeps all': 47, 'all day': 1, 'black': 4, 'just': 31, 'passed': 41, 'by': 10, 'but': 8, 'black dog': 5, 'dog just': 17, 'just passed': 32, 'passed by': 42, 'by but': 11, 'but my': 9, 'is brown': 22}\n"
     ]
    }
   ],
   "source": [
    "bigrams = text.CountVectorizer(ngram_range=(1, 2))\n",
    "print(bigrams.fit(corpus).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22992d2-79aa-4c4b-ac79-6f34aca6f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love' 'sam' 'swim' 'time']\n",
      "[[1 0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grazuole\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return stem_tokens(tokens, stemmer)\n",
    "\n",
    "vocab = ['Sam loves swimming so he swims all the time']\n",
    "vect = text.CountVectorizer(tokenizer=tokenize)\n",
    "vect.fit(vocab)\n",
    "\n",
    "sentence1 = vect.transform(['George loves swimming too!'])\n",
    "\n",
    "print(vect.get_feature_names_out())\n",
    "print(sentence1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366078f-b598-4ac5-8a4c-921796b95196",
   "metadata": {},
   "source": [
    "### Scraping text online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa66079d-53c4-4a4d-acc5-7e1eea3f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "header = {'User-Agent': 'Mozilla/5.0'}\n",
    "query = urllib2.Request(wiki, headers=header)\n",
    "page = urllib2.urlopen(query)\n",
    "soup = BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729caf01-aa57-45f7-a885-1f12e23f33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "final_table = []\n",
    "\n",
    "def extract_txt(cell):\n",
    "    cells = [c.strip() for c in cell.findAll(text=True) if '[' not in c]\n",
    "    return ' '.join(cells).strip()\n",
    "\n",
    "def filter_sq(txt):\n",
    "    return txt.split('sq')[0].strip()\n",
    "\n",
    "cols = [extract_txt(cell) for cell in table.findAll('th')]\n",
    "columns = [cols[i] for i in range(1, 7)]\n",
    "\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll('td')\n",
    "    if len(cells) > 0:\n",
    "        final_table.append([extract_txt(cells[i]) for i in range(1, 6)] + [filter_sq(extract_txt(cells[6]))])\n",
    "df = pd.DataFrame(final_table, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b33f7830-bb47-486c-ae04-c820fad0c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         City      State 2021 estimate 2020 census       Change 2020 land area\n",
      "0    New York  8,467,513     8,804,190      −3.82%  300.5 sq mi     778.3 km 2\n",
      "1  California  3,849,297     3,898,747      −1.27%  469.5 sq mi   1,216.0 km 2\n",
      "2    Illinois  2,696,555     2,746,388      −1.81%  227.7 sq mi     589.7 km 2\n",
      "3       Texas  2,288,250     2,304,580      −0.71%  640.4 sq mi   1,658.6 km 2\n",
      "4     Arizona  1,624,569     1,608,139      +1.02%  518.0 sq mi   1,341.6 km 2\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c254052-3722-43b9-b240-458e44c9885f",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c21e7e-8922-4d47-8814-afc71a7dad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Grazuole\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "shakespeare = pd.read_feather('../datasets/shakespeare_lines_in_plays.feather')\n",
    "vectorizer = text.TfidfVectorizer(max_df=1., min_df=3, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(shakespeare.lines)\n",
    "\n",
    "nmf = NMF(n_components=n, max_iter=999, random_state=101)\n",
    "nmf = nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444d5ccc-bc72-4f8b-94f5-640897784167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0: thou thy thee love ll shall come art let sir\n",
      "Topic # 1: enter exeunt exit scene ii iii iv palace attendants act\n",
      "Topic # 2: king thy lord warwick henry edward france york shall richard\n",
      "Topic # 3: caesar antony brutus cassius shall rome cleopatra egypt casca thou\n",
      "Topic # 4: antipholus dromio chain sir syracuse ephesus thou dinner home master\n",
      "Topic # 5: page ford sir master mistress shall good anne come kate\n",
      "Topic # 6: rome marcius titus lavinia andronicus marcus lucius coriolanus thy tribunes\n",
      "Topic # 7: lord good shall sir know ll man come let love\n",
      "Topic # 8: cassio iago desdemona othello moor roderigo emilia handkerchief lieutenant cyprus\n",
      "Topic # 9: hector troilus achilles ajax troy agamemnon diomed cressid patroclus pandarus\n"
     ]
    }
   ],
   "source": [
    "def print_topic_words(features, topics, top=5):\n",
    "    for i, topic in enumerate(topics):\n",
    "        words = ' '.join([features[j] for j in topic.argsort()[:-top-1:-1]])\n",
    "        print(f\"Topic #{i:2.0f}: {words}\")\n",
    "    \n",
    "print_topic_words(vectorizer.get_feature_names_out(), nmf.components_, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b98d288-9413-4731-8d49-b339700228fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamlet act: 2\n"
     ]
    }
   ],
   "source": [
    "index = shakespeare.play + ' act: ' + shakespeare.act\n",
    "\n",
    "def print_top_match(model, data, index, topic_no):\n",
    "    topic_scores = model.transform(data)[:, topic_no]\n",
    "    print(index[np.argmax(topic_scores)])\n",
    "\n",
    "print_top_match(nmf, tfidf, index, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c932c-8bda-4bc6-8189-6e5826b09f3d",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64481513-36dd-4a52-914a-c33e64f6e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The screenplay is the worst part of this film, as it lurches from one premise to the next, missing all the important bits that would have made a number of different stories possible. (This film is confusing, because the audience doesn\\'t know what the story is.) I had no problem with the low-production values and the acting wasn\\'t great, but this is telly, so it was fine. I don\\'t mind if some scenes looked like they were done in one take. But having such a non-sensical screenplay is completely unnecessary. Did any executive actually read it before forking out the cash? Avoid this at all costs.<br /><br />The prologue in particular was so poorly written, it needed a voice-over to fill in all the details that had been left out. The prologue was rushed, it wasn\\'t clear what was happening, ie. The Russian Revolution was reduced to \"Some riots are happening in Petersburg\", with the next scene being soldiers arresting them. I know the basic history of the Revolution, so I could fill in the details, \"those pesky Communists\". The prologue is best ignored.<br /><br />This could have been a thoughtful study of a person who is confused about who she is. It sets up this premise in the asylum. It could then have her struggling to identify herself for the rest of the film. No. Gone. The film assumes she is who she says she is (even though there is still no empirical evidence.) It sets up a melodramatic romance, a love so strong, it\\'ll believe anything she says. Okay, a soppy romance. No, because it makes no sense. The love interest seems like a crazed (and incidentally, sleazy) lunatic, bursting out in wild gestures. This also doesn\\'t work, because the film stupidly decides to tell the truth in the monologue at the end. They never got married and she returned to America. The love story collapses. Despite there being plenty of love scenes, I was never convinced of the reason that they were in love. I find rom-com romances more convincing, despite there only being one or two scenes which establish that they\\'ve even spent any time with each other.<br /><br />It could have been a thriller-type thing where the film assumes she is who she says she is, and she struggles to prove her identity. No, the court case is summed up rather than dealt with. The bizarre voice over comes back, again to fill in the details of a better film.<br /><br />The funniest thing to consider is what really happened. Anna Anderson was a loony who went to America and married another loony and they did crazy things together. Throughout her life, she had bouts of lunatic behaviour. None of this in the film either. There\\'s a really annoying character in the asylum who crops up from nowhere and announces herself as a \\'One Flew Over the Cuckoo\\'s Next/\\'Twelve Monkeys\\'-type informant. Thankfully, she vanishes, having brought nothing to the story.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_feather('../datasets/imdb_50k.feather')\n",
    "train = reviews.sample(30000, random_state=42)\n",
    "valid = reviews[~reviews.index.isin(train.index)].sample(10000, random_state=42)\n",
    "sampled_idx = train.index\n",
    "sampled_idx.append(valid.index)\n",
    "test = reviews[~reviews.index.isin(sampled_idx)]\n",
    "\n",
    "reviews.review.sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f089d26-1c1b-41f3-988b-d5ee8d0a3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(train.review)\n",
    "\n",
    "def tokenize_and_pad(df, tokenizer=tokenizer, maxlen=256):\n",
    "    sequences = tokenizer.texts_to_sequences(df.review)\n",
    "    padded_seqs = keras.preprocessing.sequence.pad_sequences(sequences, maxlen)\n",
    "    return padded_seqs, df.sentiment.values\n",
    "\n",
    "X, y = tokenize_and_pad(train)\n",
    "Xv, yv = tokenize_and_pad(valid)\n",
    "Xt, yt = tokenize_and_pad(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "638ca525-d54e-48d0-b016-a5ab45517621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 256, 8)            790040    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 792,089\n",
      "Trainable params: 792,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "voc = len(tokenizer.index_word) + 1\n",
    "feats = 8\n",
    "seq_len = 256\n",
    "\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Embedding(voc, feats, input_length=seq_len),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bce2eb07-f85b-49ee-a90d-dc2142300d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4231 - acc: 0.8035 - val_loss: 0.2767 - val_acc: 0.8882\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1932 - acc: 0.9280 - val_loss: 0.2602 - val_acc: 0.8952\n",
      "\n",
      "Accuracy on test set: 0.8953\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=2, batch_size=16, validation_data=(Xv, yv))\n",
    "print(f\"\\nAccuracy on test set: {model.evaluate(Xt, yt, verbose=0)[1]:.4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
